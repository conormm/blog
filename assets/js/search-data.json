{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://conormm.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Why boosting works",
            "content": "Boosting . In this post I take a look at boosting with a focus on building an intution for why this technique works. Most people who work in data science and machine learning will know that gradient boosting is one of the most powerful and effective algorithms out there. It continues to be one of the most successful ML techniques in Kaggle comps and is widely used in practice across a variety of use cases. . To build an intuition for boosting, I&#39;ll build a simple booster using the Scikit learn Decision Tree implementation. It goes without saying that the go to technique for gradient boosting is the excellent XGboost package. This post should help to develop your understanding of why boosting is so effective in predictive modelling problems. . At a high-level boosting sits within the Ensemble family of ML algorithms. Boosting involves sequentially training weak learners - where a weak learner is a low bias estimator - to predict some outcome. The interesting thing is that each learner does not predict the original tartget. Instead, each learner attempts to predict the mistakes of the previous learner. In practice this means that learner-1 will attempt to predict the target outcome directly and learner-2 will attempt to predict the residual of learner-1. This process of predicting residuals continues through to the final learner. The final prediction can then be made by taking the sum of all the individual learners. This is an extremely effective means of predicting things, but the intuition for this isn&#39;t always immediately clear. I hope to make this intuition more accessible in this post. . Let&#39;s start by importing some dependencies . from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np sns.set_style(&quot;whitegrid&quot;) . Let&#39;s create some toy data: . n = 100 X = np.linspace(0, 10, n) y = X**2 + 10 - (20 * np.random.random(n)) X = X[:, np.newaxis] plt.figure(figsize=(15, 4)) plt.scatter(X, y, alpha=.7); . The functions below create a boosted regression learner using Decision Trees. Decision trees are one of the most popular and effective learners for ensembles (though technically you boost any algorithm). Decisions trees are a nice choice however because 1) they train quickly and 2) they can model non-linearities. . I have tried to keep the functions below super simple. many_trees returns a list of decision trees, boost fits the decision trees sequentially by first predicting the target outcome y with tree-0 then from tree-1-n predicts the residuals and predict iterates through the list of fitted decision trees and returns each trees prediction. plot_fits is a convienence functions that sums the prediction of n trees and returns the fitted line and the residuals. . Implementing boosting using Decision Trees . def many_trees(n_trees, clf=False, **kwargs): trees = [DecisionTreeRegressor(**kwargs) for i in range(n_trees)] return trees def boost(trees, X, y, clf=False): fitted = [] for tree in trees: tree.fit(X, y) yhat = tree.predict(X) y = (y-yhat) fitted.append(tree) return fitted def predict(trees, X): return np.array([tree.predict(X) for tree in trees]).T . With the boosting functonality implemented, we can now fit the trees. Given the simplicity of the predicton problem, I&#39;ll make the learners extremely week by setting the max-depth of each tree to 1. This limits each tree to one split of X when predicting y. . learners = many_trees(30, max_depth=1, clf=False) fitted = boost(learners, X, y, clf=False) boosted_yhat = predict(fitted, X) xfit = np.linspace(0, 10, 100).reshape(-1, 1) #dtfit = predict(learners, xfit) # predict over the range of X to def plot_fits(n_trees, row): preds_t = boosted_yhat[:, :n_trees] boosted_pred = preds_t.sum(1) res = boosted_pred-y axes[row, 0].plot(xfit, boosted_pred, c=&#39;red&#39;) axes[row, 0].scatter(X, y) axes[row, 0].set_title(f&quot;Fit after {n_trees} trees&quot;, fontsize=15) axes[row, 1].scatter(sample_ix, res, alpha=0.7) axes[row, 1].plot(res, color=&#39;r&#39;, alpha=0.7) axes[row, 1].set_title(f&quot;Residuals after {n_trees} trees&quot;, fontsize=15) . It&#39;s the residuals, silly! . Boosting works by fitting successive models to the residuals of the previous model. It is common to plot residuals as part of the model evaluation process. Typically you check residuals to ensure that they are randon and that there are not obvious patterns. If there are patterns in the residuals it is a sign that you are missing key information about the target variable and are underfitting the data. Essentially patterns in residuals represent information about the relationship between X and y that can be modelled. . Boosting takes advantage of this insight by fitting the residuals across the sequence of models. This is why we use weak learners - such as highly regaularised decision trees - in boosting, we want each single learner to underfit the data, thereby affording the next learner the opportunity to correct its mistakes (possibly with a different sample and feature space to the previous learner). . We can see this process play out in the plots below. Each plot shows the fitted line to the data from successive boosted trees. In panel 1 we show the first prediction and it is easy to see that this leaves a clear pattern in the residuals. In the next panel we show the fit after five boosted trees. The boosting has given the model more flexibility to fit the data, but it still leaves clear exploitable patterns in the data. In the next four panels we increase the number of boosted predictions and by the final panel you can see that the residuals begin to look quite random and the line appears to be a decent fit for the data. . fig, axes = plt.subplots(nrows=6, ncols=2, figsize=(20,25)) plot_fits(1, 0) plot_fits(5, 1) plot_fits(15, 2) plot_fits(20, 3) plot_fits(25, 4) plot_fits(30, 5) fig.tight_layout() . It&#39;s also informative to plot each fit across the data by adding each succesive set of predictions and plotting the line. . plt.figure(figsize=(15, 4)) pred = 0 for i in range(len(learners)): pred += boosted_yhat[:, i] plt.plot(xfit, pred) plt.plot(xfit, predict(learners, X).sum(1)) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;y&quot;); plt.scatter(X, y, alpha=.4) . &lt;matplotlib.collections.PathCollection at 0x1a3df7ec50&gt; . There is obviously a lot more to Boosting than described in this post, but I think it is useful to have an intuitive understanding for the core reason this technique works and hopefully this post has make this clear! One final thought is that when you use Boosting you need to carefully validate your model as this approach can easily overfit. For example, if we increase the max-depth of the trees to 4, we can observe that the model begins to fit individual data points rather than the general trend in the data. . learners = many_trees(30, max_depth=4, clf=False) fitted = boost(learners, X, y, clf=False) boosted_yhat = predict(fitted, X) plt.figure(figsize=(15, 4)) plt.scatter(X, y, alpha=.4) pred = 0 for i in range(len(learners)): pred += boosted_yhat[:, i] plt.plot(xfit, pred) plt.plot(xfit, predict(learners, X).sum(1)) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;y&quot;); sample_ix = np.arange(X.shape[0]) plt.figure(figsize=(15, 4)) plt.scatter(sample_ix, pred-y) plt.plot(pred-y, color=&#39;r&#39;, alpha=0.4) . [&lt;matplotlib.lines.Line2D at 0x1a3e46fa90&gt;] . Rather than creating a randomly distributed set of residuals the model has learned to perfectly predict y given X. This might look nice, but in effect the model has become a lookup table and hasn&#39;t learned the fundamental structure of the data. A good thing to do therefore when training boosting models is to plot the residuals - this will give you a clear steer on whether or not your model is overfitting. . Hope you found this useful! .",
            "url": "https://conormm.github.io/blog/2020/02/20/boosting-intuition.html",
            "relUrl": "/2020/02/20/boosting-intuition.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "R To Py Data Wrangling",
            "content": "R to python data wrangling snippets . The dplyr package in R makes data wrangling significantly easier. The beauty of dplyr is that, by design, the options available are limited. Specifically, a set of key verbs form the core of the package. Using these verbs you can solve a wide range of data problems effectively in a shorter timeframe. Whilse transitioning to Python I have greatly missed the ease with which I can think through and solve problems using dplyr in R. The purpose of this document is to demonstrate how to execute the key dplyr verbs when manipulating data using Python (with the pandas package). . dplyr is organised around six key verbs: . . filter : subset a dataframe according to condition(s) in a variable(s) | select : choose a specific variable or set of variables | arrange : order dataframe by index or variable | group_by : create a grouped dataframe | summarise : reduce variable to summary variable (e.g. mean) | mutate : transform dataframe by adding new variables | . . The excellent pandas package in Python easily allows you to implement all of these actions (and much, much more!). Below are some snippets to highlight some of the more basic conversions. . @Conormacd . June 8th 2018 update: All of this code still works in pandas and should ease the transition from R, but for those interested in getting the most out of the package I strongly recommend this series on modern pandas https://tomaugspurger.github.io/modern-1-intro.html . Filter . R . filter(df, var &gt; 20000 &amp; var &lt; 30000) filter(df, var == &#39;string&#39;) # df %&gt;% filter(var != &#39;string&#39;) df %&gt;% filter(var != &#39;string&#39;) df %&gt;% group_by(group) %&gt;% filter(sum(var) &gt; 2000000) . Python . df[(df[&#39;var&#39;] &gt; 20000) &amp; (df[&#39;var&#39;] &lt; 30000)] df[df[&#39;var&#39;] == &#39;string&#39;] df[df[&#39;var&#39;] != &#39;string&#39;] df.groupby(&#39;group&#39;).filter(lambda x: sum(x[&#39;var&#39;]) &gt; 2000000) . Select . R . select(df, var1, var2) select(df, -var3) . Python . df[[&#39;var1&#39;, &#39;var2&#39;]] df.drop(&#39;var3&#39;, 1) . Arrange . R . arrange(df, var1) arrange(df, desc(var1)) . Python . df.sort_values(&#39;var1&#39;) df.sort_values(&#39;var1&#39;, ascending=False) . Grouping . R . df %&gt;% group_by(group) df %&gt;% group_by(group1, group2) df %&gt;% ungroup() . Python . df.groupby(&#39;group1&#39;) df.groupby([&#39;group1&#39;, &#39;group2&#39;]) df.reset_index() / or when grouping: df.groupby(&#39;group1&#39;, as_index=False) . Summarise / Aggregate df by group . R . df %&gt;% group_by(group) %&gt;% summarise(mean_var1 = mean(var1)) df %&gt;% group_by(group1, group2) %&gt;% summarise(mean_var1 = mean(var1), sum_var1 = sum(var1), count_var1 = n()) df %&gt;% group_by(group1, group2) %&gt;% summarise(mean_var1 = mean(var1), sum_2 = sum(var2), var3 = first(var3)) . Python . df.groupby(&#39;group1&#39;)[&#39;var1&#39;].agg({&#39;mean_col&#39; : np.mean()}) # pass dict to specifiy column name df.groupby([&#39;group1&#39;, &#39;group2&#39;])[&#39;var1].agg([&#39;mean&#39;, &#39;sum&#39;, &#39;count&#39;]) # for count also consider &#39;size&#39;. size will return n for NaN values also, whereas &#39;count&#39; will not. # first perform the aggregation group_agg = df.groupby([&quot;group1&quot;, &quot;group2&quot;]).agg({ &quot;var1&quot; : [&quot;mean&quot;], &quot;var2&quot; : [&quot;sum&quot;], &quot;var3&quot; : [&quot;first&quot;] }) # second rename the columns by joining the column name with the agg function (e.g. &quot;var1_mean&quot;) group_agg.columns = [&quot;_&quot;.join(x) for x in group_agg.columns.ravel()] # You can also pass multiple functions to aggregate the same column e.g: group_agg = df.groupby([&quot;group1&quot;, &quot;group2&quot;]).agg({&quot;var1&quot; : [&quot;mean&quot;, &quot;std&quot;, &quot;sum&quot;]}) . Mutate / transform df by group . R . df %&gt;% group_by(group) %&gt;% mutate(mean_var1 = mean(var1)) . Python . df.groupby(&#39;group&#39;).assign(mean_var1 = lambda x: np.mean(x.var1) . Distinct . R . df %&gt;% distinct() df %&gt;% distinct(col1) # returns dataframe with unique values of col1 . Python . df.drop_duplicates() df.drop_duplicates(subset=&#39;col1&#39;) # returns dataframe with unique values of col1 . Sample . R . sample_n(df, 100) sample_frac(df, 0.5) . Python . df.sample(100) df.sample(frac=0.5) .",
            "url": "https://conormm.github.io/blog/2020/01/26/R-to-Py-data-wrangling.html",
            "relUrl": "/2020/01/26/R-to-Py-data-wrangling.html",
            "date": " • Jan 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://conormm.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data scientist, currently working at Amazon in London. I don’t blog as frequently as I would like but I’m going to try to post short-form tutorials and other interesting data science pieces here. Feel free to look around, comment and enjoy! . Conor . This website is powered by fastpages [^1]. .",
          "url": "https://conormm.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://conormm.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}