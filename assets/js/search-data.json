{
  
    
        "post0": {
            "title": "Tail events, why they matter and how to model them (code)",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&quot;whitegrid&quot;) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:90% !important; }&lt;/style&gt;&quot;)) . plt.figure(figsize=(20, 12)) np.random.seed(123) n = 100000 fs = {&quot;fontsize&quot; : 16} sns.kdeplot(stats.distributions.gumbel_r().rvs(n), label=&quot;Gumbel&quot;) sns.kdeplot(stats.distributions.norm().rvs(n), label=&quot;Normal&quot;) plt.legend(**fs) plt.xticks(**fs) plt.yticks(**fs) plt.ylabel(&quot;Density&quot;, **fs) plt.xlabel(&quot;X&quot;, **fs); . df = pd.read_csv( &quot;/Users/conormcdonald/Downloads/archive-5/DailyDelhiClimateTrain.csv&quot;, parse_dates=[&quot;date&quot;] ) . block_maxima_month = ( df .set_index(&quot;date&quot;) .resample(&quot;M&quot;) .max() .wind_speed .reset_index() .query(&quot;wind_speed &gt; 5&quot;) ) fs = {&quot;fontsize&quot; : 16} plt.figure(figsize=(24, 10)) plt.scatter(df.date, df.wind_speed, alpha=.3) plt.plot(df.date, df.wind_speed, alpha=.7, c=&quot;b&quot;) plt.scatter(block_maxima_month.date, block_maxima_month.wind_speed, c=&quot;red&quot;, s=80) plt.xticks(**fs) plt.yticks(**fs) plt.ylabel(&quot;Wind speed&quot;, **fs) plt.xlabel(&quot;Date&quot;, **fs); . plt.figure(figsize=(24, 10)) sns.distplot(block_maxima_month.wind_speed) plt.xticks(**fs) plt.yticks(**fs) plt.ylabel(&quot;Density&quot;, **fs) plt.xlabel(&quot;Wind speed&quot;, **fs); . /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . ev_model = pm.Model() with ev_model: loc = pm.Normal(&quot;loc&quot;, 5, 10) scale = pm.HalfCauchy(&quot;scale&quot;, 1) lik = pm.Gumbel(&quot;lik&quot;, loc, scale, observed=block_maxima_month.wind_speed) ev_trace = pm.sample() norm_model = pm.Model() with norm_model: loc = pm.Normal(&quot;loc&quot;, 5, 10) scale = pm.HalfCauchy(&quot;scale&quot;, 1) lik = pm.Normal(&quot;lik&quot;, loc, scale, observed=block_maxima_month.wind_speed) norm_trace = pm.sample() t_model = pm.Model() with t_model: loc = pm.Normal(&quot;loc&quot;, 5, 10) scale = pm.HalfCauchy(&quot;scale&quot;, 1) nu = pm.Exponential(&quot;nu&quot;, 20) lik = pm.StudentT(&quot;lik&quot;, nu, loc, scale, observed=block_maxima_month.wind_speed) t_trace = pm.sample() . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [scale, loc] . . 100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [scale, loc] . . 100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds. The acceptance probability does not match the target. It is 0.8800705684725215, but should be close to 0.8. Try to increase the number of tuning steps. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, scale, loc] . . 100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds. . p1 = pm.traceplot(ev_trace, figsize=(14, 7)) . /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, . p2 = pm.traceplot(norm_trace, figsize=(14, 7)); . /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, . pm.plot_posterior(ev_trace) pm.plot_posterior(norm_trace) . /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, . array([&lt;AxesSubplot:title={&#39;center&#39;:&#39;loc&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;scale&#39;}&gt;], dtype=object) . ev_post = pm.sample_posterior_predictive(ev_trace, model=ev_model)[&quot;lik&quot;].T norm_post = pm.sample_posterior_predictive(norm_trace, model=norm_model)[&quot;lik&quot;].T t_post = pm.sample_posterior_predictive(t_trace, model=t_model)[&quot;lik&quot;].T . . 100.00% [4000/4000 00:01&lt;00:00] . 100.00% [4000/4000 00:03&lt;00:00] . 100.00% [4000/4000 00:03&lt;00:00] ev_post = pm.sample_posterior_predictive(ev_trace, model=ev_model)[&quot;lik&quot;].T norm_post = pm.sample_posterior_predictive(norm_trace, model=norm_model)[&quot;lik&quot;].T # Plot density plt.figure(figsize=(20, 10)) pm.plot_dist(ev_post, label=&quot;Gumbel&quot;, textsize=20, hist_kwargs=fs) pm.plot_dist(norm_post, label=&quot;Normal&quot;, textsize=20, ) sns.kdeplot(block_maxima_month.wind_speed, label=&quot;Actual&quot;) plt.legend(fontsize=14) . &lt;matplotlib.legend.Legend at 0x7fd72bd41e10&gt; . def calcualte_exceedance_probability(exceed_values, posterior): n = posterior.shape[1] ix = [] probs_m = [] probs_05 = [] probs_95 = [] for i in exceed_values: p = ((posterior&gt;i).sum(1)/n) p05 = np.quantile(p, 0.01) p95 = np.quantile(p, 0.99) probs_m.append(p.mean()) probs_05.append(p05) probs_95.append(p95) ix.append(i) return pd.DataFrame(dict(exceed_value=ix, ep_mean=probs_m, ep_lb=probs_05, ep_ub=probs_95)) exceedance = calcualte_exceedance_probability(list(range(1, 60)), ev_post) plt.figure(figsize=(20, 12)) plt.plot(exceedance.exceed_value, exceedance.ep_mean, c=&quot;r&quot;, label=&quot;ev&quot;) plt.fill_between(exceedance.exceed_value, exceedance.ep_lb, exceedance.ep_ub, color=&quot;b&quot;, alpha=.3) plt.xticks(**fs) plt.yticks(**fs) plt.ylabel(&quot;Wind speed&quot;, **fs) plt.xlabel(&quot;Date&quot;, **fs); plt.legend() . &lt;matplotlib.legend.Legend at 0x7fd7329cc890&gt; .",
            "url": "https://conormm.github.io/blog/2021/05/09/tail-events,-bayesian-approaches.html",
            "relUrl": "/2021/05/09/tail-events,-bayesian-approaches.html",
            "date": " • May 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fantastic features and where to find them (code)",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor from sklearn import datasets sns.set_style(&quot;whitegrid&quot;) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:90% !important; }&lt;/style&gt;&quot;)) . data = [ (datasets.load_boston, &quot;reg&quot;), (datasets.load_breast_cancer, &quot;cls&quot;), (datasets.load_diabetes, &quot;reg&quot;), (datasets.load_digits, &quot;cls&quot;), (datasets.load_iris, &quot;cls&quot;), (datasets.load_linnerud, &quot;reg&quot;), (datasets.load_wine, &quot;cls&quot;), ] . importances = [] for f, target in data: X, y = f(return_X_y=True) model = RandomForestRegressor() if target == &quot;reg&quot; else RandomForestClassifier() model.fit(X, y) feat_nb = model.feature_importances_ feat_nb = np.sort(feat_nb, axis=-1)[::-1] importances.append(feat_nb) . importances_df = pd.DataFrame(importances, index=[&quot;boston&quot;, &quot;cancer&quot;, &quot;diabetes&quot;, &quot;digits&quot;, &quot;iris&quot;, &quot;linnerud&quot;, &quot;wine&quot;]) . importances_df.T.plot(figsize=(30, 14)) . &lt;AxesSubplot:&gt; . importances_df.cumsum(1).T.plot(figsize=(30, 14)) . &lt;AxesSubplot:&gt; .",
            "url": "https://conormm.github.io/blog/2021/04/24/fantastic-features.html",
            "relUrl": "/2021/04/24/fantastic-features.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Earnings on Medium (code)",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm import pymc3 as pm sns.set_style(&quot;whitegrid&quot;) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:90% !important; }&lt;/style&gt;&quot;)) df = pd.read_csv(&quot;/Users/conormcdonald/Desktop/medium_post_data.csv&quot;, skiprows=1).iloc[:, 1:-4] df = df[~df.internal_views.isnull()] h = df.member_reading_time.astype(str).str.split(&quot;.&quot;, expand=True) h.loc[:, 0] = (h.loc[:, 0].astype(float) * 60)/60 h.loc[:, 1] = (&quot;.&quot; + h.loc[:, 1]).astype(float) h.loc[:, 1] = (h.loc[:, 1] * 100)/60 df[&quot;member_reading_time_adj&quot;] = h.sum(1) plt.figure(figsize=(14, 8)) plt.scatter(np.log(df.member_reading_time_adj), np.log(df.earnings), alpha=.5) m = sm.OLS(endog=df.earnings, exog=sm.add_constant(df[[&quot;member_reading_time_adj&quot;]])) fitted = m.fit() fitted.summary() plt.figure(figsize=(14, 8)) plt.scatter(df.member_reading_time_adj, df.earnings, alpha=.4, color=&quot;red&quot;) pd.to_datetime(&quot;2021-04-20&quot;)-pd.to_datetime(&quot;2017-11-27&quot;) plt.figure(figsize=(14, 7)) np.random.seed(123) x = np.clip(sm.add_constant(np.random.poisson(lam=4.61, size=1240)), 0, np.Inf) plt.plot(x[:, 1], c=&quot;green&quot;, alpha=.5) plt.ylabel(&quot;Member reading time&quot;, fontsize=16) plt.xlabel(&quot;Days since publishing&quot;, fontsize=16) x[:, 1].sum()/60 fitted.predict(x).sum() df[&quot;yhat&quot;] = fitted.predict(sm.add_constant(df.member_reading_time_adj)) df.assign(error = lambda x: x.earnings-x.yhat).error.mean() ## Bayesian Model hours_model = pm.Model() with hours_model: a = pm.Normal(&quot;a&quot;, 0, 1) b = pm.Normal(&quot;b&quot;, 0, 1) error = pm.HalfCauchy(&quot;error&quot;, 3) yhat = a + b * df.member_reading_time_adj lik = pm.Normal(&quot;lik&quot;, yhat, error, observed=df.earnings) with hours_model: trace = pm.sample(2000, chains=4) plt.figure(figsize=(14, 7)) pm.plot_dist(trace[&quot;b&quot;]) plt.xlabel(&quot;Model estimate of earnings $&quot;, fontsize=16) plt.ylabel(&quot;Probability density&quot;, fontsize=16) plt.xticks(fontsize=14) plt.yticks(fontsize=14); pm.traceplot(trace); pm.summary(trace) preds = pm.sample_posterior_predictive(trace, model=hours_model, samples=2000) plt.figure(figsize=(14, 8)) plt.scatter(df.member_reading_time_adj, df.earnings, alpha=.4, color=&quot;blue&quot;) plt.plot(df.member_reading_time_adj, pd.DataFrame(preds[&quot;lik&quot;]).mean(), alpha=.8, color=&quot;red&quot;, linewidth=3) plt.xlabel(&quot;Member reading time (hrs)&quot;, fontsize=16) plt.ylabel(&quot;Earnings $&quot;, fontsize=16) plt.xticks(fontsize=14) plt.yticks(fontsize=14); pm.trace_to_dataframe(trace) views_model = pm.Model() with views_model: a = pm.Normal(&quot;a&quot;, 0, 1) b = pm.Normal(&quot;b&quot;, 0, 1) error = pm.HalfCauchy(&quot;error&quot;, 3) yhat = a + b * df.internal_views lik = pm.Normal(&quot;lik&quot;, yhat, error, observed=df.earnings) with views_model: trace = pm.sample(2000, chains=4) plt.figure(figsize=(14, 7)) pm.plot_dist(trace[&quot;b&quot;]) plt.xlabel(&quot;Model estimate of earnings $&quot;, fontsize=16) plt.ylabel(&quot;Probability density&quot;, fontsize=16) plt.xticks(fontsize=14) plt.yticks(fontsize=14); pm.traceplot(trace); pm.summary(trace) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [error, b, a] . . 100.00% [12000/12000 00:04&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 5 seconds. /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/pymc3/sampling.py:1618: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample &#34;samples parameter is smaller than nchains times ndraws, some draws &#34; . . 100.00% [2000/2000 00:02&lt;00:00] Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [error, b, a] . . 100.00% [12000/12000 00:07&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 7 seconds. /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, /Users/conormcdonald/opt/anaconda3/lib/python3.7/site-packages/arviz/data/io_pymc3.py:89: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. FutureWarning, . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . a 0.389 | 0.206 | 0.016 | 0.791 | 0.003 | 0.002 | 5749.0 | 5298.0 | 5773.0 | 5585.0 | 1.0 | . b 0.020 | 0.001 | 0.017 | 0.022 | 0.000 | 0.000 | 6077.0 | 6077.0 | 6092.0 | 5587.0 | 1.0 | . error 1.323 | 0.137 | 1.075 | 1.573 | 0.002 | 0.001 | 5469.0 | 5469.0 | 5363.0 | 4415.0 | 1.0 | .",
            "url": "https://conormm.github.io/blog/2021/04/20/earnings.html",
            "relUrl": "/2021/04/20/earnings.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Lesser known data science techniques you should add to your toolkit (code)",
            "content": "from sklearn.base import BaseEstimator, RegressorMixin from sklearn.base import ClassifierMixin import statsmodels.api as sm import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np sns.set_style(&quot;whitegrid&quot;) get_ipython().run_line_magic(&#39;matplotlib&#39;, &#39;inline&#39;) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;&quot;)) . n = 100 X = np.linspace(0, 30, n) y = 150 + 0.3*X + 1*X**2 + np.random.normal(loc=50, scale=100, size=n) X = X[:, np.newaxis] plt.figure(figsize=(17, 7)) plt.scatter(X, y, alpha=.7); . class QuantileRegression(BaseEstimator, RegressorMixin): &quot;&quot;&quot;Sklearn wrapper for statsmodels Quantile Regression &quot;&quot;&quot; def __init__(self, quantile=0.5, **kwargs): self.quantile = quantile self.kwargs = kwargs self.model = None self.fitted = None def fit(self, X, y=None): X = sm.add_constant(X) self.model = sm.QuantReg(endog=y, exog=X, **self.kwargs) self.fitted = self.model.fit(q=self.quantile) def predict(self, X, y=None): X = sm.add_constant(X) return self.fitted.predict(X) . n = 100 X = np.linspace(0, 30, n) y = 150 + 0.3*X + 1*X**2 + np.random.normal(loc=50, scale=100, size=n) X = X[:, np.newaxis] plt.figure(figsize=(17, 7)) plt.scatter(X, y, alpha=.7); # instantiate models qr_05 = QuantileRegression(.05) qr_50 = QuantileRegression(quantile=.5) qr_95 = QuantileRegression(quantile=.95) # structure data for model model_input = np.c_[X, X**2] # fit models qr_05.fit(model_input, y) qr_50.fit(model_input, y) qr_95.fit(model_input, y) . qr.fitted.summary() . QuantReg Regression Results Dep. Variable: y | Pseudo R-squared: 0.6992 | . Model: QuantReg | Bandwidth: 104.1 | . Method: Least Squares | Sparsity: 268.4 | . Date: Sat, 10 Apr 2021 | No. Observations: 100 | . Time: 17:29:23 | Df Residuals: 97 | . | Df Model: 2 | . | coef std err t P&gt;|t| [0.025 0.975] . const 185.1674 | 39.463 | 4.692 | 0.000 | 106.844 | 263.491 | . x1 8.3116 | 6.080 | 1.367 | 0.175 | -3.755 | 20.378 | . x2 0.7510 | 0.196 | 3.830 | 0.000 | 0.362 | 1.140 | . The condition number is large, 1.2e+03. This might indicate that there arestrong multicollinearity or other numerical problems. plt.figure(figsize=(25, 13)) plt.scatter(X, y) preds_05 = qr_05.predict(model_input) preds_50 = qr_50.predict(model_input) preds_95 = qr_95.predict(model_input) plt.plot(X, preds_05, color=&quot;orange&quot;, label=&quot;0.05 Quantile&quot;) plt.plot(X, preds_50, color=&quot;red&quot;, label=&quot;0.5 Quantile&quot;) plt.plot(X, preds_95, color=&quot;green&quot;, label=&quot;0.95 Quantile&quot;) plt.fill_between(X.reshape(-1,), preds_05.reshape(-1, ), preds_95.reshape(-1, ), color=&quot;blue&quot;, alpha=.3) fs = 30 plt.legend(fontsize=fs/2) plt.title(&quot;Quantile Regression Predictions&quot;, fontsize=fs) plt.xlabel(&quot;X&quot;, fontsize=fs) plt.ylabel(&quot;y&quot;, fontsize=fs) plt.yticks(fontsize=fs) plt.xticks(fontsize=fs); . medium_post_views = np.array([3500, 1600, 482, 245, 198, 116]) days_since_launch = np.arange(medium_post_views.shape[0]) . plt.figure(figsize=(25, 13)) plt.plot(medium_post_views) plt.scatter(days_since_launch, medium_post_views) plt.title(&quot;Medium views decay&quot;, fontsize=fs) plt.ylabel(&quot;Views&quot;, fontsize=fs) plt.yticks(fontsize=fs) plt.xlabel(&quot;Days since launch&quot;, fontsize=fs) plt.xticks(fontsize=fs); . from scipy.optimize import curve_fit from sklearn.base import BaseEstimator, RegressorMixin import statsmodels.api as sm class ExponentialDecayRegressor(BaseEstimator, RegressorMixin): &quot;&quot;&quot;Fits an exponential decay curve &quot;&quot;&quot; def __init__(self, starting_values=[1.,1.e-5,1.], **kwargs,): self.starting_values = starting_values self.kwargs = kwargs self.params = None def fit(self, X, y=None): self.params, _ = curve_fit(self.exp_decay_f, X, y, p0=self.starting_values) def predict(self, X, y=None): return self.exp_decay_f(X, *self.params) @staticmethod def exp_decay_f(X, a, k, b): return a * np.exp(-k*X) + b . medium_post_views = np.array([3500, 1600, 482, 245, 198, 116]) days_since_launch = np.arange(medium_post_views.shape[0]) xd = ExponentialDecayRegressor() xd.fit(days_since_launch, medium_post_views) days_since_launch_plus_future = np.arange(12) xd_preds = xd.predict(days_since_launch_pred) . plt.figure(figsize=(25, 13)) plt.plot(days_since_launch_plus_future, xd_preds, label=&quot;exponential decay fit&quot;) plt.plot(days_since_launch, medium_post_views, label=&quot;actual&quot;) plt.legend(fontsize=fs/2) plt.title(&quot;Medium views decay&quot;, fontsize=fs) plt.ylabel(&quot;Views&quot;, fontsize=fs) plt.yticks(fontsize=fs) plt.xlabel(&quot;Days since launch&quot;, fontsize=fs) plt.xticks(fontsize=fs); . medium_post_views/medium_post_views.max() . array([1. , 0.45714286, 0.13771429, 0.07 , 0.05657143, 0.03314286]) . xd = ExponentialDecayRegressor(starting_values=None) xd.fit(days_since_launch, medium_post_views/medium_post_views.max()) days_since_launch_plus_future = np.arange(12) xd_preds = xd.predict(days_since_launch_pred) . plt.figure(figsize=(25, 13)) plt.plot(days_since_launch_plus_future, xd_preds, label=&quot;exponential decay fit&quot;) plt.plot(days_since_launch, medium_post_views/medium_post_views.max(), label=&quot;actual&quot;) plt.legend(fontsize=fs/2) plt.title(&quot;Medium views decay&quot;, fontsize=fs) plt.ylabel(&quot;Views&quot;, fontsize=fs) plt.yticks(fontsize=fs) plt.xlabel(&quot;Days since launch&quot;, fontsize=fs) plt.xticks(fontsize=fs); . from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.datasets import load_breast_cancer, load_diabetes . rf = GradientBoostingClassifier(n_estimators=30, max_depth=4) . X, y = load_breast_cancer(return_X_y=True) train_ix, test_ix = train_test_split(np.arange(X.shape[0])) train_X, train_y = X[train_ix], y[train_ix] test_X, test_y = X[test_ix], y[test_ix] . from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.ensemble import GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import OneHotEncoder class TreeEmbeddingLogisticRegression(BaseEstimator, ClassifierMixin): &quot;&quot;&quot;Fits a logistic regression model on tree embeddings. &quot;&quot;&quot; def __init__(self, **kwargs): self.kwargs = kwargs self.gbm = GradientBoostingClassifier(**kwargs) self.lr = LogisticRegression(penalty=&quot;l1&quot;, solver=&quot;liblinear&quot;) self.bin = OneHotEncoder() def fit(self, X, y=None): self.gbm.fit(X, y) X_emb = self.gbm.apply(X).reshape(X.shape[0], -1) X_emb = self.bin.fit_transform(X_emb) self.lr.fit(X_emb, y) def predict(self, X, y=None, with_tree=False): if with_tree: preds = self.gbm.predict(X) else: X_emb = self.gbm.apply(X).reshape(X.shape[0], -1) X_emb = self.bin.transform(X_emb) preds = self.lr.predict(X_emb) return preds def predict_proba(self, X, y=None, with_tree=False): if with_tree: preds = self.gbm.predict_proba(X) else: X_emb = self.gbm.apply(X).reshape(X.shape[0], -1) X_emb = self.bin.transform(X_emb) preds = self.lr.predict_proba(X_emb) return preds . lr_tree = TreeEmbeddingLogisticRegression(n_estimators=30, max_depth=4) . lr_tree.fit(train_X, train_y) . lr_tree.X_emb.toarray().shape . (426, 440) . train_X.shape . (426, 30) . lr_tree.X_emb . &lt;426x440 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 12780 stored elements in Compressed Sparse Row format&gt; . lr_tree_preds = lr_tree.predict(test_X) tree_preds = lr_tree.predict(test_X, with_tree=True) . metrics.roc_auc_score(test_y, lr_tree_preds) . 0.9444799658994033 . from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin class CustomClassifier(BaseEstimator, ClassifierMixin): def __init__(self): pass def fit(self, X, y=None): pass def predict(self, X, y=None, with_tree=False): pass def predict_proba(self, X, y=None, with_tree=False): pass class CustomRegressor(BaseEstimator, RegressorMixin): def __init__(self): pass def fit(self, X, y=None): pass def predict(self, X, y=None, with_tree=False): pass def predict_proba(self, X, y=None, with_tree=False): pass .",
            "url": "https://conormm.github.io/blog/2021/04/11/lesser-known-data-scienec-techniques.html",
            "relUrl": "/2021/04/11/lesser-known-data-scienec-techniques.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How to optimize your webpage with simple Python code (code)",
            "content": "import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from scipy.stats import beta n_trials = 10000 sns.set_style(&quot;whitegrid&quot;) get_ipython().run_line_magic(&#39;matplotlib&#39;, &#39;inline&#39;) from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:80% !important; }&lt;/style&gt;&quot;)) . class Environment: &quot;&quot;&quot;Class for simulating an experiment in whichi multiple options are presented to users. For simulation purposes we set the expected payout to let the sampling algorithm find the optimal option across n_trials. &quot;&quot;&quot; def __init__(self, options, payouts, n_trials): self.options = options self.payouts = payouts self.n_trials = n_trials self.total_reward = 0 self.n_options = len(options) self.shape = (self.n_options, n_trials) def run(self, agent): &quot;&quot;&quot;Run the simulation with the agent. agent must be a class with choose_option and update methods.&quot;&quot;&quot; for i in range(self.n_trials): # agent makes a choice x_chosen = agent.choose_option() # Environment returns reward # In a real setting this wouldn&#39;t exist and the model # would be updated directly using the number of successes (presented and clickthrough event) # and failures (presented and no clickthrough event) reward = np.random.binomial(1, p=self.payouts[x_chosen]) # agent learns of reward agent.reward = reward # agent updates parameters based on the data agent.update() self.total_reward += reward agent.collect_data() return self.total_reward def plot_k_choices(agent, env): cmap = plt.get_cmap(&quot;tab10&quot;, env.n_options) x = np.arange(0, agent.n_trials) plt.figure(figsize=(30, 12)) plt.scatter(x, agent.option_i.round().astype(int)+1, cmap=cmap, c=agent.option_i.round().astype(int)+1, marker=&quot;.&quot;, alpha=1) plt.title(agent, fontsize=22, fontweight=&quot;bold&quot;) plt.xlabel(&quot;Trial&quot;, fontsize=22, fontweight=&quot;bold&quot;) plt.xticks(fontsize=15) plt.ylabel(&quot;Option&quot;, fontsize=22, fontweight=&quot;bold&quot;) plt.yticks(fontsize=15) plt.yticks(np.array(range(env.n_options))+1) plt.colorbar(); . class ThompsonSampler: &quot;&quot;&quot;Thompson Sampling using Beta distribution associated with each option. The beta distribution will be updated when rewards associated with each option are observed. &quot;&quot;&quot; def __init__(self, env, n_learning=0): # boilier plate data storage self.env = env self.n_learning = n_learning self.options = env.options self.n_trials = env.n_trials self.payouts = env.payouts self.option_i = np.zeros(env.n_trials) self.r_i = np.zeros(env.n_trials) self.thetas = np.zeros(self.n_trials) self.data = None self.reward = 0 self.total_reward = 0 self.option = 0 self.trial = 0 # parameters of beta distribution self.alpha = np.ones(env.n_options) self.beta = np.ones(env.n_options) # estimated payout rates self.theta = np.zeros(env.n_options) def collect_data(self): self.data = pd.DataFrame(dict(option=self.option_i, reward=self.r_i)) self.n_learning = n_learning def choose_option(self): # sample from posterior (this is the thompson sampling approach) # this leads to more exploration because machines with &gt; uncertainty can then be selected as the machine self.theta = np.random.beta(self.alpha, self.beta) # select machine with highest posterior p of payout if self.trial &lt; self.n_learning: self.option = np.random.choice(self.options) else: self.option = self.options[np.argmax(self.theta)] return self.option def update(self): #update dist (a, b) = (a, b) + (r, 1 - r) # a,b are the alpha, beta parameters of a Beta distribution self.alpha[self.option] += self.reward # i.e. only increment b when it&#39;s a swing and a miss. 1 - 0 = 1, 1 - 1 = 0 self.beta[self.option] += 1 - self.reward # store the option presented on each trial self.thetas[self.trial] = self.theta[self.option] self.option_i[self.trial] = self.option self.r_i[self.trial] = self.reward self.trial += 1 def collect_data(self): self.data = pd.DataFrame(dict(option=self.option_i, reward=self.r_i)) def __str__(self): return &quot;ThompsonSampler&quot; . options_and_payouts = { &quot;option-1&quot; : 0.07, &quot;option-2&quot; : 0.05, &quot;option-3&quot; : 0.04, &quot;option-4&quot; : 0.01 } . machines = [0, 1, 2, 3] payouts = [0.07, 0.05, 0.04, 0.01] labels = [&quot;V&quot; + str(i) + (str(p)) for i, p in zip(machines, payouts)] assert len(machines) == len(payouts) . en2 = Environment(machines, payouts, 1000) tsa = ThompsonSampler(env=en2) en2.run(agent=tsa) tsa.data.option.value_counts() . 0.0 823 2.0 74 3.0 52 1.0 51 Name: option, dtype: int64 . plt.figure(figsize=(14, 7)) plot_k_choices(tsa, en2); . &lt;Figure size 1008x504 with 0 Axes&gt; . x = np.arange(0, .2, 0.0001) cmap = list(plt.cm.tab10(list(range(len(machines))))) plt.figure(figsize=(40, 20)) # plot 1 n_rounds = 0 en = Environment(machines, payouts, n_rounds) tsa = ThompsonSampler(env=en) plt.subplot(231) for i in range(len(machines)): pdf = beta(tsa.alpha[i], tsa.beta[i]).pdf(x) c = cmap[i] plt.plot(x, pdf, c=c, label=i+1, alpha=.6) plt.title(f&quot;Prior distribution for each variant (uniform between 0 and 1)&quot;) plt.legend(); # plot 2 n_rounds = 500 en = Environment(machines, payouts, n_rounds) tsa = ThompsonSampler(env=en) en.run(agent=tsa) plt.subplot(232) for i in range(len(machines)): pdf = beta(tsa.alpha[i], tsa.beta[i]).pdf(x) c = cmap[i] plt.plot(x, pdf, c=c, label=i+1, alpha=.6) plt.title(f&quot;Probability distributions after {n_rounds}&quot;) plt.legend(); # plot 3 en = Environment(machines, payouts, n_rounds) tsa = ThompsonSampler(env=en) en.run(agent=tsa) n_rounds = 1000 plt.subplot(233) for i in range(len(machines)): pdf = beta(tsa.alpha[i], tsa.beta[i]).pdf(x) c = cmap[i] plt.plot(x, pdf, c=c, label=i+1, alpha=.6) plt.title(f&quot;Probability distributions after {n_rounds}&quot;) plt.legend(); .",
            "url": "https://conormm.github.io/blog/2021/04/10/thompson-samping.html",
            "relUrl": "/2021/04/10/thompson-samping.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Why boosting works",
            "content": "Boosting . In this post I take a look at boosting with a focus on building an intution for why this technique works. Most people who work in data science and machine learning will know that gradient boosting is one of the most powerful and effective algorithms out there. It continues to be one of the most successful ML techniques in Kaggle comps and is widely used in practice across a variety of use cases. . To build an intuition for boosting, I&#39;ll build a simple booster using the Scikit learn Decision Tree implementation. It goes without saying that the go to technique for gradient boosting is the excellent XGboost package. This post should help to develop your understanding of why boosting is so effective in predictive modelling problems. . At a high-level boosting sits within the Ensemble family of ML algorithms. Boosting involves sequentially training weak learners - where a weak learner is a low bias estimator - to predict some outcome. The interesting thing is that each learner does not predict the original tartget. Instead, each learner attempts to predict the mistakes of the previous learner. In practice this means that learner-1 will attempt to predict the target outcome directly and learner-2 will attempt to predict the residual of learner-1. This process of predicting residuals continues through to the final learner. The final prediction can then be made by taking the sum of all the individual learners. This is an extremely effective means of predicting things, but the intuition for this isn&#39;t always immediately clear. I hope to make this intuition more accessible in this post. . Let&#39;s start by importing some dependencies . from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np sns.set_style(&quot;whitegrid&quot;) . Let&#39;s create some toy data: . n = 100 X = np.linspace(0, 10, n) y = X**2 + 10 - (20 * np.random.random(n)) X = X[:, np.newaxis] plt.figure(figsize=(15, 4)) plt.scatter(X, y, alpha=.7); . The functions below create a boosted regression learner using Decision Trees. Decision trees are one of the most popular and effective learners for ensembles (though technically you boost any algorithm). Decisions trees are a nice choice however because 1) they train quickly and 2) they can model non-linearities. . I have tried to keep the functions below super simple. many_trees returns a list of decision trees, boost fits the decision trees sequentially by first predicting the target outcome y with tree-0 then from tree-1-n predicts the residuals and predict iterates through the list of fitted decision trees and returns each trees prediction. plot_fits is a convienence functions that sums the prediction of n trees and returns the fitted line and the residuals. . Implementing boosting using Decision Trees . def many_trees(n_trees, clf=False, **kwargs): trees = [DecisionTreeRegressor(**kwargs) for i in range(n_trees)] return trees def boost(trees, X, y, clf=False): fitted = [] for tree in trees: tree.fit(X, y) yhat = tree.predict(X) y = (y-yhat) fitted.append(tree) return fitted def predict(trees, X): return np.array([tree.predict(X) for tree in trees]).T . With the boosting functonality implemented, we can now fit the trees. Given the simplicity of the predicton problem, I&#39;ll make the learners extremely week by setting the max-depth of each tree to 1. This limits each tree to one split of X when predicting y. . learners = many_trees(30, max_depth=1, clf=False) fitted = boost(learners, X, y, clf=False) boosted_yhat = predict(fitted, X) xfit = np.linspace(0, 10, 100).reshape(-1, 1) #dtfit = predict(learners, xfit) # predict over the range of X to def plot_fits(n_trees, row): preds_t = boosted_yhat[:, :n_trees] boosted_pred = preds_t.sum(1) res = boosted_pred-y axes[row, 0].plot(xfit, boosted_pred, c=&#39;red&#39;) axes[row, 0].scatter(X, y) axes[row, 0].set_title(f&quot;Fit after {n_trees} trees&quot;, fontsize=15) axes[row, 1].scatter(sample_ix, res, alpha=0.7) axes[row, 1].plot(res, color=&#39;r&#39;, alpha=0.7) axes[row, 1].set_title(f&quot;Residuals after {n_trees} trees&quot;, fontsize=15) . It&#39;s the residuals, silly! . Boosting works by fitting successive models to the residuals of the previous model. It is common to plot residuals as part of the model evaluation process. Typically you check residuals to ensure that they are randon and that there are not obvious patterns. If there are patterns in the residuals it is a sign that you are missing key information about the target variable and are underfitting the data. Essentially patterns in residuals represent information about the relationship between X and y that can be modelled. . Boosting takes advantage of this insight by fitting the residuals across the sequence of models. This is why we use weak learners - such as highly regaularised decision trees - in boosting, we want each single learner to underfit the data, thereby affording the next learner the opportunity to correct its mistakes (possibly with a different sample and feature space to the previous learner). . We can see this process play out in the plots below. Each plot shows the fitted line to the data from successive boosted trees. In panel 1 we show the first prediction and it is easy to see that this leaves a clear pattern in the residuals. In the next panel we show the fit after five boosted trees. The boosting has given the model more flexibility to fit the data, but it still leaves clear exploitable patterns in the data. In the next four panels we increase the number of boosted predictions and by the final panel you can see that the residuals begin to look quite random and the line appears to be a decent fit for the data. . fig, axes = plt.subplots(nrows=6, ncols=2, figsize=(20,25)) plot_fits(1, 0) plot_fits(5, 1) plot_fits(15, 2) plot_fits(20, 3) plot_fits(25, 4) plot_fits(30, 5) fig.tight_layout() . It&#39;s also informative to plot each fit across the data by adding each succesive set of predictions and plotting the line. . plt.figure(figsize=(15, 4)) pred = 0 for i in range(len(learners)): pred += boosted_yhat[:, i] plt.plot(xfit, pred) plt.plot(xfit, predict(learners, X).sum(1)) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;y&quot;); plt.scatter(X, y, alpha=.4) . &lt;matplotlib.collections.PathCollection at 0x1a3df7ec50&gt; . There is obviously a lot more to Boosting than described in this post, but I think it is useful to have an intuitive understanding for the core reason this technique works and hopefully this post has make this clear! One final thought is that when you use Boosting you need to carefully validate your model as this approach can easily overfit. For example, if we increase the max-depth of the trees to 4, we can observe that the model begins to fit individual data points rather than the general trend in the data. . learners = many_trees(30, max_depth=4, clf=False) fitted = boost(learners, X, y, clf=False) boosted_yhat = predict(fitted, X) plt.figure(figsize=(15, 4)) plt.scatter(X, y, alpha=.4) pred = 0 for i in range(len(learners)): pred += boosted_yhat[:, i] plt.plot(xfit, pred) plt.plot(xfit, predict(learners, X).sum(1)) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;y&quot;); sample_ix = np.arange(X.shape[0]) plt.figure(figsize=(15, 4)) plt.scatter(sample_ix, pred-y) plt.plot(pred-y, color=&#39;r&#39;, alpha=0.4) . [&lt;matplotlib.lines.Line2D at 0x1a3e46fa90&gt;] . Rather than creating a randomly distributed set of residuals the model has learned to perfectly predict y given X. This might look nice, but in effect the model has become a lookup table and hasn&#39;t learned the fundamental structure of the data. A good thing to do therefore when training boosting models is to plot the residuals - this will give you a clear steer on whether or not your model is overfitting. . Hope you found this useful! .",
            "url": "https://conormm.github.io/blog/2020/02/20/boosting-intuition.html",
            "relUrl": "/2020/02/20/boosting-intuition.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "R To Py Data Wrangling",
            "content": "R to python data wrangling snippets . The dplyr package in R makes data wrangling significantly easier. The beauty of dplyr is that, by design, the options available are limited. Specifically, a set of key verbs form the core of the package. Using these verbs you can solve a wide range of data problems effectively in a shorter timeframe. Whilse transitioning to Python I have greatly missed the ease with which I can think through and solve problems using dplyr in R. The purpose of this document is to demonstrate how to execute the key dplyr verbs when manipulating data using Python (with the pandas package). . dplyr is organised around six key verbs: . . filter : subset a dataframe according to condition(s) in a variable(s) | select : choose a specific variable or set of variables | arrange : order dataframe by index or variable | group_by : create a grouped dataframe | summarise : reduce variable to summary variable (e.g. mean) | mutate : transform dataframe by adding new variables | . . The excellent pandas package in Python easily allows you to implement all of these actions (and much, much more!). Below are some snippets to highlight some of the more basic conversions. . @Conormacd . June 8th 2018 update: All of this code still works in pandas and should ease the transition from R, but for those interested in getting the most out of the package I strongly recommend this series on modern pandas https://tomaugspurger.github.io/modern-1-intro.html . Filter . R . filter(df, var &gt; 20000 &amp; var &lt; 30000) filter(df, var == &#39;string&#39;) # df %&gt;% filter(var != &#39;string&#39;) df %&gt;% filter(var != &#39;string&#39;) df %&gt;% group_by(group) %&gt;% filter(sum(var) &gt; 2000000) . Python . df[(df[&#39;var&#39;] &gt; 20000) &amp; (df[&#39;var&#39;] &lt; 30000)] df[df[&#39;var&#39;] == &#39;string&#39;] df[df[&#39;var&#39;] != &#39;string&#39;] df.groupby(&#39;group&#39;).filter(lambda x: sum(x[&#39;var&#39;]) &gt; 2000000) . Select . R . select(df, var1, var2) select(df, -var3) . Python . df[[&#39;var1&#39;, &#39;var2&#39;]] df.drop(&#39;var3&#39;, 1) . Arrange . R . arrange(df, var1) arrange(df, desc(var1)) . Python . df.sort_values(&#39;var1&#39;) df.sort_values(&#39;var1&#39;, ascending=False) . Grouping . R . df %&gt;% group_by(group) df %&gt;% group_by(group1, group2) df %&gt;% ungroup() . Python . df.groupby(&#39;group1&#39;) df.groupby([&#39;group1&#39;, &#39;group2&#39;]) df.reset_index() / or when grouping: df.groupby(&#39;group1&#39;, as_index=False) . Summarise / Aggregate df by group . R . df %&gt;% group_by(group) %&gt;% summarise(mean_var1 = mean(var1)) df %&gt;% group_by(group1, group2) %&gt;% summarise(mean_var1 = mean(var1), sum_var1 = sum(var1), count_var1 = n()) df %&gt;% group_by(group1, group2) %&gt;% summarise(mean_var1 = mean(var1), sum_2 = sum(var2), var3 = first(var3)) . Python . df.groupby(&#39;group1&#39;)[&#39;var1&#39;].agg({&#39;mean_col&#39; : np.mean()}) # pass dict to specifiy column name df.groupby([&#39;group1&#39;, &#39;group2&#39;])[&#39;var1].agg([&#39;mean&#39;, &#39;sum&#39;, &#39;count&#39;]) # for count also consider &#39;size&#39;. size will return n for NaN values also, whereas &#39;count&#39; will not. # first perform the aggregation group_agg = df.groupby([&quot;group1&quot;, &quot;group2&quot;]).agg({ &quot;var1&quot; : [&quot;mean&quot;], &quot;var2&quot; : [&quot;sum&quot;], &quot;var3&quot; : [&quot;first&quot;] }) # second rename the columns by joining the column name with the agg function (e.g. &quot;var1_mean&quot;) group_agg.columns = [&quot;_&quot;.join(x) for x in group_agg.columns.ravel()] # You can also pass multiple functions to aggregate the same column e.g: group_agg = df.groupby([&quot;group1&quot;, &quot;group2&quot;]).agg({&quot;var1&quot; : [&quot;mean&quot;, &quot;std&quot;, &quot;sum&quot;]}) . Mutate / transform df by group . R . df %&gt;% group_by(group) %&gt;% mutate(mean_var1 = mean(var1)) . Python . df.groupby(&#39;group&#39;).assign(mean_var1 = lambda x: np.mean(x.var1) . Distinct . R . df %&gt;% distinct() df %&gt;% distinct(col1) # returns dataframe with unique values of col1 . Python . df.drop_duplicates() df.drop_duplicates(subset=&#39;col1&#39;) # returns dataframe with unique values of col1 . Sample . R . sample_n(df, 100) sample_frac(df, 0.5) . Python . df.sample(100) df.sample(frac=0.5) .",
            "url": "https://conormm.github.io/blog/2020/01/26/R-to-Py-data-wrangling.html",
            "relUrl": "/2020/01/26/R-to-Py-data-wrangling.html",
            "date": " • Jan 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data scientist, currently working at Amazon in London. I don’t blog as frequently as I would like but I’m going to try to post short-form tutorials and other interesting data science pieces here. Feel free to look around, comment and enjoy! . Conor . This website is powered by fastpages [^1]. .",
          "url": "https://conormm.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://conormm.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}